{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will decode the current choice of the animal given information from the past 10 trials. In a previous step, we reduced the number of features to the 30 most relevant ones. We will now train a neural network on these reduced inputs. First, we import relevant modules and define useful functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ROOT = '/Users/pablomartin/python'\n",
    "folder = ROOT + '/DATA_structures/featureSelection/'\n",
    "original = ROOT + '/DATA_structures/'\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def predict_NN(NN, X, Y):\n",
    "    Y_prediction = NN.predict(X)\n",
    "    Y_prediction = (Y_prediction > 0.5)\n",
    "    return float(np.sum(Y_prediction.ravel() == Y))/len(Y)\n",
    "\n",
    "def train_NN(X,Y, hidden_layers, BS, EPS = 50):\n",
    "\n",
    "    #Initializing Neural Network\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    # Adding input layer \n",
    "    classifier.add(Dense(units = hidden_layers['1'],\n",
    "                         kernel_initializer = 'uniform',\n",
    "                         activation = 'relu',\n",
    "                         input_dim = X.shape[1]))\n",
    "\n",
    "    # Adding hidden layer\n",
    "    for layer in hidden_layers.keys():\n",
    "        if hidden_layers[layer] > 0:\n",
    "            classifier.add(Dense(units = hidden_layers[layer],\n",
    "                                 kernel_initializer = 'uniform',\n",
    "                                 activation = 'relu'))\n",
    "\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = 1,\n",
    "                         kernel_initializer = 'uniform',\n",
    "                         activation = 'sigmoid'))\n",
    "    \n",
    "    # Compiling Neural Network\n",
    "    classifier.compile(optimizer = 'adam',\n",
    "                       loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy'])\n",
    "\n",
    "    classifier.fit(X, Y, batch_size = BS, epochs = EPS, verbose=0)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's give it a go. Randomly picking the number of units in the hidden layer to be 20, and the training batch size to be 20, we will see how a perceptron decodes the choices of rats. We will divide the data into training and testing sets as usual, EXCEPT, that we will separate it by leaving 2 sessions aside as the testing set and train on the remainding data. We want to be able to see where in the session the neural networks are not decoding properly. If we divide using the usual train_test_split section, we will get tons of holes in our sessions. Most datasets have ~11 sessions, so leaving 2 sessions out is equivalent to leaving 20% of the data out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training network...PSR_TbyT_Saline.p\n",
      "finished training. elapsed time: 0.50\n",
      "score:0.685\n",
      "training network...PSR_TbyT_FirstTraining.p\n",
      "finished training. elapsed time: 0.20\n",
      "score:0.682\n",
      "training network...PSR_TbyT_MPFC.p\n",
      "finished training. elapsed time: 0.26\n",
      "score:0.706\n",
      "training network...PSR_TbyT_Ipsi.p\n",
      "finished training. elapsed time: 0.27\n",
      "score:0.588\n",
      "training network...DSR_TbyT_Saline.p\n",
      "finished training. elapsed time: 0.51\n",
      "score:0.780\n",
      "training network...DSR_TbyT_Ipsi.p\n",
      "finished training. elapsed time: 0.30\n",
      "score:0.700\n",
      "training network...PSR_TbyT_Contra.p\n",
      "finished training. elapsed time: 0.41\n",
      "score:0.546\n",
      "training network...DSR_TbyT_FirstTraining.p\n",
      "finished training. elapsed time: 0.23\n",
      "score:0.629\n",
      "training network...DSR_TbyT_MPFC.p\n",
      "finished training. elapsed time: 0.29\n",
      "score:0.711\n",
      "training network...PSR_TbyT_OFC.p\n",
      "finished training. elapsed time: 0.30\n",
      "score:0.599\n",
      "training network...PSR_TbyT_MidTraining.p\n",
      "finished training. elapsed time: 0.37\n",
      "score:0.596\n",
      "training network...DSR_TbyT_MidTraining.p\n",
      "finished training. elapsed time: 0.52\n",
      "score:0.759\n",
      "training network...DSR_TbyT_OFC.p\n",
      "finished training. elapsed time: 0.30\n",
      "score:0.700\n",
      "training network...DSR_TbyT_Contra.p\n",
      "finished training. elapsed time: 0.29\n",
      "score:0.649\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "#for starters\n",
    "unitNo = {'1': 30, '2': 5}\n",
    "batchSize = 100\n",
    "    \n",
    "feature_labels = [str(X.columns.levels[0][a]) + '_' +  str(X.columns.levels[1][b]) \\\n",
    "             for a,b in zip(X.columns.labels[0], X.columns.labels[1])]   \n",
    "        \n",
    "\n",
    "networks = []\n",
    "datasets = []\n",
    "for fileName in os.listdir(folder):\n",
    "\n",
    "\n",
    "    #loading reduced data\n",
    "    X,y = pickle.load(open(folder + '/' + fileName, 'rb'))\n",
    "    \n",
    "    noSessions = range(len(X.index.levels[0]))\n",
    "    random.shuffle(noSessions)\n",
    "    \n",
    "    #we must pick 2 sessions randomly  \n",
    "    train_sessions = idx[['S' + str(w) for w in noSessions[2:]], :, :]\n",
    "    test_sessions = idx[['S' + str(w) for w in noSessions[:2]], :, :]\n",
    "    \n",
    "    training_dataset = [X.loc[train_sessions, :], y.loc[train_sessions]]\n",
    "    testing_dataset = [X.loc[test_sessions, :], y.loc[test_sessions]]\n",
    "    data_splits = [training_dataset, testing_dataset]\n",
    "    #want to save each datasplit\n",
    "    datasets.append(data_splits)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    print 'training network...%s' %(fileName)\n",
    "    NN = train_NN(training_dataset[0], training_dataset[1], unitNo, batchSize)\n",
    "    print 'finished training. elapsed time: %1.2f' %((time.time() - start) / 60)\n",
    "    \n",
    "    #we want to save each NN to play around with later\n",
    "    networks.append(NN)\n",
    "    #quick score\n",
    "    print 'score:%1.3f' %predict_NN(NN, testing_dataset[0], testing_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go and optimize the neural network, we want to get a sense of how it's decoding trials. We want to build the tools to visualize this anyways. We want a function that given a session, it graphs where the reversals were and where the decoder made a mistake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def plot_decoder_accuracy(y, decoder, title = 'Learning Curve'):\n",
    "    noTrials = y.shape[0]\n",
    "    moving_window = 5\n",
    "    moving_average = np.zeros(noTrials - moving_window, dtype=float)\n",
    "    for trial in range(noTrials - moving_window):\n",
    "        moving_average[trial] = np.nanmean(y.iloc[trial : trial + moving_window])\n",
    "        \n",
    "    #applying Savitzky-Golay filter to smooth curve\n",
    "    moving_average = savgol_filter(moving_average, moving_window, 2)\n",
    "    \n",
    "    blocks = y.groupby(axis = 0, level = 'block')\n",
    "    revPoints = [len(b) for label, b in blocks]\n",
    "    revPoints = np.cumsum(revPoints)\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    \n",
    "    #plotting learning curve\n",
    "    ax.plot(range(noTrials - moving_window), moving_average)\n",
    "    \n",
    "    #plotting reversal block points\n",
    "    for xc in revPoints:\n",
    "        plt.axvline(x = xc, color='green')\n",
    "        \n",
    "    #what trials did neural network miscode\n",
    "    trials_decoded = (decoder == y)    \n",
    "    misDecoded = np.nonzero(trials_decoded == 0)[0]\n",
    "    #plot as red dot on top of learning curve\n",
    "    ax.scatter(misDecoded, [0.5] * len(misDecoded), marker='o', c='red')\n",
    "    \n",
    "    \n",
    "    ax.set_ylabel('P(choosing W)', FontSize=12)\n",
    "    ax.set_xlabel('trial #', FontSize=12)\n",
    "    ax.set_title(title, FontSize = 14)\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "for datasetIndex, fileName in enumerate(os.listdir(folder)):   \n",
    "\n",
    "    training_dataset, testing_dataset = datasets[datasetIndex]    \n",
    "\n",
    "    #session data\n",
    "    iterator = testing_dataset[0].groupby(axis=0, level='session')\n",
    "    #current choice\n",
    "    y_ = testing_dataset[1].groupby(axis=0, level='session')\n",
    "\n",
    "    currentNN = networks[datasetIndex]\n",
    "\n",
    "    for session, y in zip(iterator, y_):\n",
    "        #running neural network on session\n",
    "        decoder = (currentNN.predict(session[1]) > 0.5).ravel()\n",
    "        print 'score for session: %1.3f' %(float(np.sum(decoder == y[1])) / len(y[1]))\n",
    "\n",
    "        #creating label for session\n",
    "        title = fileName + '_' + y[0]\n",
    "        #plotting session\n",
    "        plot_decoder_accuracy(y[1], decoder, title = title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will optimize each neural network. Since they are very slow to train, we will only explore a limited feature space. We will start by separating each dataset into three sets: training, validating, and testing. We will use the training set to (duh) train the neural network and to test it on the validation set. A final unbiased test will be performed on the testing set. \n",
    "\n",
    "The feature space will consists of varying three variables:  \n",
    "1) number of units in hidden layers  \n",
    "2) whether there is 1 or 2 hidden layers  \n",
    "3) batch size    \n",
    "\n",
    "Unfortunately, since neural networks are probabilistic, we will need to train 3 times per condition and average result. We will try the following parameters:  \n",
    "\n",
    "1) [5, 10, 30, 100] # of units  \n",
    "2) [1, 2] # of hidden layers\n",
    "3) [10, 50, 100] batch size    \n",
    "\n",
    "We are looking at 4x2x3x3 = 72 networks to train per dataset. It takes ~ 2 minutes to trian each network, so we are looking at 33 hours processing time total without parallelization. Accross 4 cores it *should* be around 8 hours. We will try to do this overnight. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: PSR_TbyT_Ipsi.p\n",
      "\n",
      "# of training examples: 780\n",
      "# of validation set examples: 195\n",
      "# of examples in testing set: 292\n",
      "\n",
      "\n",
      "NN 1/60 finished - processing time: 15.04 minutes - score:0.638\n",
      "NN 2/60 finished - processing time: 4.51 minutes - score:0.634\n",
      "NN 3/60 finished - processing time: 3.72 minutes - score:0.629\n",
      "NN 4/60 finished - processing time: 8.24 minutes - score:0.593\n",
      "NN 5/60 finished - processing time: 4.39 minutes - score:0.621\n",
      "NN 6/60 finished - processing time: 5.05 minutes - score:0.631\n",
      "NN 7/60 finished - processing time: 7.78 minutes - score:0.574\n",
      "NN 8/60 finished - processing time: 4.39 minutes - score:0.573\n",
      "NN 9/60 finished - processing time: 3.70 minutes - score:0.609\n",
      "NN 10/60 finished - processing time: 7.79 minutes - score:0.556\n",
      "NN 11/60 finished - processing time: 4.20 minutes - score:0.528\n",
      "NN 12/60 finished - processing time: 3.54 minutes - score:0.540\n",
      "NN 13/60 finished - processing time: 8.33 minutes - score:0.574\n",
      "NN 14/60 finished - processing time: 4.28 minutes - score:0.636\n",
      "NN 15/60 finished - processing time: 4.22 minutes - score:0.598\n",
      "NN 16/60 finished - processing time: 8.89 minutes - score:0.619\n",
      "NN 17/60 finished - processing time: 7.20 minutes - score:0.624\n",
      "NN 18/60 finished - processing time: 6.10 minutes - score:0.626\n",
      "NN 19/60 finished - processing time: 10.47 minutes - score:0.583\n",
      "NN 20/60 finished - processing time: 5.69 minutes - score:0.610\n",
      "NN 21/60 finished - processing time: 4.35 minutes - score:0.624\n",
      "NN 22/60 finished - processing time: 8.94 minutes - score:0.568\n",
      "NN 23/60 finished - processing time: 4.82 minutes - score:0.564\n",
      "NN 24/60 finished - processing time: 4.46 minutes - score:0.581\n",
      "NN 25/60 finished - processing time: 8.97 minutes - score:0.627\n",
      "NN 26/60 finished - processing time: 5.12 minutes - score:0.636\n",
      "NN 27/60 finished - processing time: 4.59 minutes - score:0.631\n",
      "NN 28/60 finished - processing time: 8.34 minutes - score:0.576\n",
      "NN 29/60 finished - processing time: 4.77 minutes - score:0.629\n",
      "NN 30/60 finished - processing time: 4.16 minutes - score:0.622\n",
      "NN 31/60 finished - processing time: 8.26 minutes - score:0.573\n",
      "NN 32/60 finished - processing time: 5.06 minutes - score:0.579\n",
      "NN 33/60 finished - processing time: 4.77 minutes - score:0.600\n",
      "NN 34/60 finished - processing time: 8.76 minutes - score:0.593\n",
      "NN 35/60 finished - processing time: 5.48 minutes - score:0.562\n",
      "NN 36/60 finished - processing time: 4.84 minutes - score:0.578\n",
      "NN 37/60 finished - processing time: 8.89 minutes - score:0.607\n",
      "NN 38/60 finished - processing time: 5.40 minutes - score:0.621\n",
      "NN 39/60 finished - processing time: 4.99 minutes - score:0.624\n",
      "NN 40/60 finished - processing time: 9.05 minutes - score:0.598\n",
      "NN 41/60 finished - processing time: 5.52 minutes - score:0.590\n",
      "NN 42/60 finished - processing time: 5.12 minutes - score:0.619\n",
      "NN 43/60 finished - processing time: 9.26 minutes - score:0.550\n",
      "NN 44/60 finished - processing time: 5.67 minutes - score:0.602\n",
      "NN 45/60 finished - processing time: 5.66 minutes - score:0.591\n",
      "NN 46/60 finished - processing time: 9.39 minutes - score:0.547\n",
      "NN 47/60 finished - processing time: 5.76 minutes - score:0.549\n",
      "NN 48/60 finished - processing time: 5.32 minutes - score:0.568\n",
      "NN 49/60 finished - processing time: 9.36 minutes - score:0.622\n",
      "NN 50/60 finished - processing time: 5.89 minutes - score:0.632\n",
      "NN 51/60 finished - processing time: 5.34 minutes - score:0.639\n",
      "NN 52/60 finished - processing time: 9.69 minutes - score:0.576\n",
      "NN 53/60 finished - processing time: 6.20 minutes - score:0.614\n",
      "NN 54/60 finished - processing time: 5.39 minutes - score:0.610\n",
      "NN 55/60 finished - processing time: 9.80 minutes - score:0.557\n",
      "NN 56/60 finished - processing time: 6.28 minutes - score:0.586\n",
      "NN 57/60 finished - processing time: 5.64 minutes - score:0.571\n",
      "NN 58/60 finished - processing time: 10.28 minutes - score:0.523\n",
      "NN 59/60 finished - processing time: 6.30 minutes - score:0.554\n",
      "NN 60/60 finished - processing time: 6.01 minutes - score:0.540\n",
      "[[ 0.63760684  0.63418803  0.62905983]\n",
      " [ 0.59316239  0.62051282  0.63076923]\n",
      " [ 0.57435897  0.57264957  0.60854701]\n",
      " [ 0.55555556  0.52820513  0.54017094]\n",
      " [ 0.57435897  0.63589744  0.5982906 ]\n",
      " [ 0.61880342  0.62393162  0.62564103]\n",
      " [ 0.58290598  0.61025641  0.62393162]\n",
      " [ 0.56752137  0.56410256  0.58119658]\n",
      " [ 0.62735043  0.63589744  0.63076923]\n",
      " [ 0.57606838  0.62905983  0.62222222]\n",
      " [ 0.57264957  0.57948718  0.6       ]\n",
      " [ 0.59316239  0.56239316  0.57777778]\n",
      " [ 0.60683761  0.62051282  0.62393162]\n",
      " [ 0.5982906   0.58974359  0.61880342]\n",
      " [ 0.55042735  0.6017094   0.59145299]\n",
      " [ 0.54700855  0.54871795  0.56752137]\n",
      " [ 0.62222222  0.63247863  0.63931624]\n",
      " [ 0.57606838  0.61367521  0.61025641]\n",
      " [ 0.55726496  0.58632479  0.57094017]\n",
      " [ 0.52307692  0.55384615  0.54017094]]\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "max score: 0.639\n",
      "units layer 1: 5\n",
      "units layer 2: 100\n",
      "batch size: 100\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "dataset: DSR_TbyT_Saline.p\n",
      "\n",
      "# of training examples: 2940\n",
      "# of validation set examples: 735\n",
      "# of examples in testing set: 187\n",
      "\n",
      "\n",
      "NN 1/60 finished - processing time: 23.76 minutes - score:0.755\n",
      "NN 2/60 finished - processing time: 28.68 minutes - score:0.760\n",
      "NN 3/60 finished - processing time: 11.66 minutes - score:0.753\n",
      "NN 4/60 finished - processing time: 31.63 minutes - score:0.756\n",
      "NN 5/60 finished - processing time: 15.64 minutes - score:0.757\n",
      "NN 6/60 finished - processing time: 9.58 minutes - score:0.759\n",
      "NN 7/60 finished - processing time: 24.17 minutes - score:0.744\n",
      "NN 8/60 finished - processing time: 9.41 minutes - score:0.752\n",
      "NN 9/60 finished - processing time: 7.84 minutes - score:0.758\n",
      "NN 10/60 finished - processing time: 24.01 minutes - score:0.736\n",
      "NN 11/60 finished - processing time: 9.77 minutes - score:0.744\n",
      "NN 12/60 finished - processing time: 7.85 minutes - score:0.748\n",
      "NN 13/60 finished - processing time: 24.57 minutes - score:0.759\n",
      "NN 14/60 finished - processing time: 9.86 minutes - score:0.679\n",
      "NN 15/60 finished - processing time: 8.20 minutes - score:0.755\n",
      "NN 16/60 finished - processing time: 25.35 minutes - score:0.676\n",
      "NN 17/60 finished - processing time: 10.31 minutes - score:0.762\n",
      "NN 18/60 finished - processing time: 8.50 minutes - score:0.756\n",
      "NN 19/60 finished - processing time: 25.80 minutes - score:0.765\n",
      "NN 20/60 finished - processing time: 10.45 minutes - score:0.755\n",
      "NN 21/60 finished - processing time: 8.68 minutes - score:0.759\n",
      "NN 22/60 finished - processing time: 26.01 minutes - score:0.747\n",
      "NN 23/60 finished - processing time: 11.44 minutes - score:0.759\n",
      "NN 24/60 finished - processing time: 9.68 minutes - score:0.759\n",
      "NN 25/60 finished - processing time: 27.25 minutes - score:0.760\n",
      "NN 26/60 finished - processing time: 12.52 minutes - score:0.759\n",
      "NN 27/60 finished - processing time: 11.71 minutes - score:0.757\n",
      "NN 28/60 finished - processing time: 28.80 minutes - score:0.762\n",
      "NN 29/60 finished - processing time: 13.31 minutes - score:0.757\n",
      "NN 30/60 finished - processing time: 11.48 minutes - score:0.762\n",
      "NN 31/60 finished - processing time: 29.66 minutes - score:0.755\n",
      "NN 32/60 finished - processing time: 13.61 minutes - score:0.756\n",
      "NN 33/60 finished - processing time: 11.83 minutes - score:0.761\n",
      "NN 34/60 finished - processing time: 30.86 minutes - score:0.740\n",
      "NN 35/60 finished - processing time: 14.40 minutes - score:0.756\n",
      "NN 36/60 finished - processing time: 12.78 minutes - score:0.761\n",
      "NN 37/60 finished - processing time: 34.73 minutes - score:0.760\n",
      "NN 38/60 finished - processing time: 19.24 minutes - score:0.761\n",
      "NN 39/60 finished - processing time: 15.75 minutes - score:0.757\n",
      "NN 40/60 finished - processing time: 37.18 minutes - score:0.759\n",
      "NN 41/60 finished - processing time: 17.95 minutes - score:0.759\n",
      "NN 42/60 finished - processing time: 16.55 minutes - score:0.759\n",
      "NN 43/60 finished - processing time: 40.26 minutes - score:0.752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#let's split the data into three sets but with testing set being 2 entire sessions\n",
    "fileNames = os.listdir(folder)\n",
    "for fileName in fileNames[3:]:\n",
    "\n",
    "    scores = np.full([20, 3], np.NaN)\n",
    "\n",
    "    #loading reduced data\n",
    "    X,y = pickle.load(open(folder + '/' + fileName, 'rb'))\n",
    "    \n",
    "    noSessions = range(len(X.index.levels[0]))\n",
    "    random.shuffle(noSessions)\n",
    "    \n",
    "    #we must pick 2 sessions randomly  \n",
    "    train_sessions = idx[['S' + str(w) for w in noSessions[2:]], :, :]\n",
    "    test_sessions = idx[['S' + str(w) for w in noSessions[:2]], :, :]\n",
    "    \n",
    "    training_dataset = [X.loc[train_sessions, :], y.loc[train_sessions]]\n",
    "    testing_dataset = [X.loc[test_sessions, :], y.loc[test_sessions]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(training_dataset[0],\n",
    "                                                                training_dataset[1],\n",
    "                                                                test_size=0.20,\n",
    "                                                                random_state = 23)\n",
    "\n",
    "    data_splits = [training_dataset, testing_dataset]\n",
    "    print 'dataset: %s\\n' %fileName\n",
    "    print '# of training examples: %i' %X_train.shape[0]\n",
    "    print '# of validation set examples: %i' %X_validate.shape[0]\n",
    "    print '# of examples in testing set: %i\\n\\n' %testing_dataset[0].shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    units = [5, 10, 30, 100]\n",
    "    layer1 = [w for s in range(5) for w in units]\n",
    "    layer2 = [w for w in [-1] + units for s in range(4)]\n",
    "    batchSizes = [10, 50, 100]\n",
    "    for rIndex, (units1, units2) in enumerate(zip(layer1, layer2)):\n",
    "        #unit structure\n",
    "        unitNo = {'1': units1, '2' : units2}\n",
    "        for batchIndex, batchSize in enumerate(batchSizes):\n",
    "            \n",
    "            #train neural network\n",
    "            start = time.time()\n",
    "            \n",
    "            tmp_score = []\n",
    "            for shuffle in range(3):\n",
    "                NN = train_NN(X_train, y_train, unitNo, batchSize)\n",
    "                \n",
    "                #calculate score on validation set\n",
    "                tmp_score.append(predict_NN(NN, X_validate, y_validate))\n",
    "                #tmp_score.append(np.random.random())\n",
    "            \n",
    "            print 'NN %i/%i finished - processing time: %1.2f minutes - score:%1.3f' \\\n",
    "                 %(1 + rIndex * 3 + batchIndex, 60, ((time.time() - start) / 60), np.nanmean(tmp_score))    \n",
    "            scores[rIndex, batchIndex] = np.nanmean(tmp_score)\n",
    "    \n",
    "    print scores\n",
    "    max_condition = np.nonzero(scores == np.max(scores))\n",
    "    model_params = (layer1[max_condition[0][0]], layer2[max_condition[0][0]], batchSizes[max_condition[1][0]])\n",
    "    print '\\n\\n'\n",
    "    print '*' * 80\n",
    "    print 'max score: %1.3f' %np.max(scores)\n",
    "    print 'units layer 1: %i\\nunits layer 2: %i\\nbatch size: %i' %model_params\n",
    "    print '*' * 80\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
